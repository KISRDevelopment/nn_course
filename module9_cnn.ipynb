{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2447269e-fb72-47f9-b6e8-a4985cbc1d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3994d086-2d85-4040-933c-04ecd6e88814",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "* So far we've built fully connected feed-forward (FF) NNs, where each input node is connected to each hidden node.\n",
    "* A fully connected NN that has a 30 unit hidden layer and accepts 28x28 images has $28 \\times 28 \\times 30 + 30 = 23,550$ weights between the input and hidden layer\n",
    "* The number of weights will quickly grow if we start dealing with larger color images which have more pixels and where each pixel has three numbers associated with it (red, blue, and green)\n",
    "* The network will quickly overfit as a result\n",
    "* For images, the standard way to handle this problem is via the convolutional neural network which consists of convolutional layers, pooling layers, and the usual fully-connected layers.\n",
    "* Those networks have drastically fewer parameters than FF NNs.\n",
    "\n",
    "## Convolution Layer\n",
    "\n",
    "The convolution layer makes two assumptions about its inputs:\n",
    "\n",
    "1. Inputs that are nearby are related.\n",
    "2. A detector that could detect a pattern in (x, y) can be used to detect the same pattern in other locations in the image\n",
    "\n",
    "Both assumptions are very reasonable in images: pixels that are nearby are likely to share statistical properties and a detector that can detect edges at the top of the image, can also detect edges at the bottom.\n",
    "\n",
    "### One Dimensional Convolution\n",
    "Let's see how basic convolution operator works in the 1D input case (recall that images are 2D). A single neuron in a convolution layer defines a receptive field that operates over a limited range of inputs. The neuron slides this receptive field over the input to produce the final outputs:\n",
    "\n",
    "![alt text](figures/1dconv.jpg)\n",
    "\n",
    "In the Figure, the neuron (opaque magenta) defines a simple operation over the sequence: it computes the average of pairs of neighbors. At the first step, the neuron computes the average of the first two elements. At the second step the neuron computes the average of the second and third elements, and so on. \n",
    "\n",
    "The neuron has two weights: 0.5 and 0.5 which mean that both inputs are weighted equally, so it computes a straightforward average. But we can change the weights to anything, and, as you might have guessed, those weights will be free parameters of a convolutional neural network. \n",
    "\n",
    "It is important to note that the *same weights* are applied across the entire input.\n",
    "\n",
    "Here's an example of how one might implement 1D convolution in numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c90cd92-5e1a-4874-b03f-13ce89c2bcff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.5,  3. ,  4.5,  6. ,  7.5,  9. , 10.5, 12. ,  8.5])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here's a 1D \"image\"\n",
    "input_data = np.array([1,2,3,4,5,6,7,8,9])\n",
    "\n",
    "# a convolutional layer neuron defines\n",
    "convolution_kernel = np.array([0.5, 0.5, 0.5])\n",
    "\n",
    "y_same = np.convolve(input_data, convolution_kernel, \"same\")\n",
    "\n",
    "y_same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1da62e-e5e2-4fe1-bcab-418dcbe62533",
   "metadata": {},
   "source": [
    "The `\"same\"` argument tells numpy to add $m-1$ zeros at the left of the sequence (where $m$ is the size of the convolution kernel, 3 in this case) so that the output of the convolution operation has the same size as the input. There are other padding options:\n",
    "\n",
    "* `\"full\"`: adds $m-1$ zeros at both ends of the sequence. The final output has size $n + m - 1$.\n",
    "* `\"valid\"`: computes entries where the kernel and the input sequence fully overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "326884ee-7e2a-4bdc-9a47-6e8be2ee31c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6 14 34 34  8]\n",
      "[14 34 34]\n"
     ]
    }
   ],
   "source": [
    "x = [6, 2]\n",
    "h = [1, 2, 5, 4]\n",
    "\n",
    "y = np.convolve(h, x, \"full\")\n",
    "print(y)\n",
    "\n",
    "y = np.convolve(h, x, \"valid\")\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4815c7b-a36b-43fb-9499-8ffaf2089331",
   "metadata": {},
   "source": [
    "We call the convolution operation with a specific kernel (weights) a filter.\n",
    "\n",
    "### Two Dimensional Convolution\n",
    "\n",
    "https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks\n",
    "\n",
    "# Example (Simple MNIST convnet from Keras Documentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29e37c65-7089-4568-9dc3-1bb26b2ebd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfd16929-8d02-4e47-af06-627b6dfd27c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0ceda88-19ed-43e6-9b1d-1634bee14d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([1,2,3])\n",
    "print(y.shape)\n",
    "np.expand_dims(y, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9540b71-9143-439e-a87e-a67c12c425b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ddc91f6-76e5-4206-9db2-1f595f6b62f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                16010     \n",
      "=================================================================\n",
      "Total params: 34,826\n",
      "Trainable params: 34,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b62da97b-046b-46bf-858e-9cbcc7a0939f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "422/422 [==============================] - 11s 27ms/step - loss: 0.3667 - accuracy: 0.8885 - val_loss: 0.0819 - val_accuracy: 0.9783\n",
      "Epoch 2/15\n",
      "422/422 [==============================] - 12s 27ms/step - loss: 0.1115 - accuracy: 0.9657 - val_loss: 0.0569 - val_accuracy: 0.9838\n",
      "Epoch 3/15\n",
      "422/422 [==============================] - 11s 27ms/step - loss: 0.0825 - accuracy: 0.9747 - val_loss: 0.0458 - val_accuracy: 0.9885\n",
      "Epoch 4/15\n",
      "422/422 [==============================] - 12s 29ms/step - loss: 0.0686 - accuracy: 0.9787 - val_loss: 0.0414 - val_accuracy: 0.9893\n",
      "Epoch 5/15\n",
      "422/422 [==============================] - 12s 27ms/step - loss: 0.0619 - accuracy: 0.9812 - val_loss: 0.0377 - val_accuracy: 0.9905\n",
      "Epoch 6/15\n",
      "422/422 [==============================] - 11s 27ms/step - loss: 0.0569 - accuracy: 0.9824 - val_loss: 0.0349 - val_accuracy: 0.9900\n",
      "Epoch 7/15\n",
      "422/422 [==============================] - 12s 28ms/step - loss: 0.0516 - accuracy: 0.9839 - val_loss: 0.0333 - val_accuracy: 0.9915\n",
      "Epoch 8/15\n",
      "422/422 [==============================] - 12s 28ms/step - loss: 0.0460 - accuracy: 0.9855 - val_loss: 0.0331 - val_accuracy: 0.9917\n",
      "Epoch 9/15\n",
      "422/422 [==============================] - 12s 28ms/step - loss: 0.0430 - accuracy: 0.9863 - val_loss: 0.0361 - val_accuracy: 0.9893\n",
      "Epoch 10/15\n",
      "422/422 [==============================] - 12s 28ms/step - loss: 0.0410 - accuracy: 0.9872 - val_loss: 0.0283 - val_accuracy: 0.9925\n",
      "Epoch 11/15\n",
      "422/422 [==============================] - 12s 27ms/step - loss: 0.0383 - accuracy: 0.9879 - val_loss: 0.0308 - val_accuracy: 0.9915\n",
      "Epoch 12/15\n",
      "422/422 [==============================] - 12s 27ms/step - loss: 0.0380 - accuracy: 0.9877 - val_loss: 0.0285 - val_accuracy: 0.9932\n",
      "Epoch 13/15\n",
      "422/422 [==============================] - 12s 28ms/step - loss: 0.0356 - accuracy: 0.9884 - val_loss: 0.0284 - val_accuracy: 0.9930\n",
      "Epoch 14/15\n",
      "422/422 [==============================] - 12s 28ms/step - loss: 0.0349 - accuracy: 0.9884 - val_loss: 0.0288 - val_accuracy: 0.9923\n",
      "Epoch 15/15\n",
      "422/422 [==============================] - 12s 28ms/step - loss: 0.0316 - accuracy: 0.9900 - val_loss: 0.0280 - val_accuracy: 0.9917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x181c907daf0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 15\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9f74818-2fb9-42ca-ba61-df45aa4c2921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.024911930784583092\n",
      "Test accuracy: 0.9911999702453613\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94781cd8-3e58-4fb0-98ed-239163cfc98e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
