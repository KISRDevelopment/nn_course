{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "aba58628-95f9-4b1f-a2e7-4e9add7ea587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.model_selection\n",
    "import numpy.random as rng\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import sklearn.metrics\n",
    "import tensorflow.keras.regularizers as reg \n",
    "import itertools\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df505fe-7117-4492-9586-833cd595f0b0",
   "metadata": {},
   "source": [
    "# Module 9: Practical Matters\n",
    "\n",
    "## Cross-Validation\n",
    "\n",
    "- So far, we estimated the performance of a model under one random training and testing split.\n",
    "- This can be problematic as we can luck into a particularly good split and vice-versa. \n",
    "- The different random initializations of the weights on the NN can also introduce extra variability.\n",
    "- A better approach would be to evaluate the classifier under different random splits and average the result (e.g., to get average MSE)\n",
    "- An even better approach is to use _cross-validation_, which results in less biased estimates of the error.\n",
    "\n",
    "- CV shuffles the dataset and splits it into $N$ sets of non-overlapping _folds_ (usually $N=5$)\n",
    "- The model is trained and evaluated $N$ times, in each time it is trained on $N-1$ folds (green boxes) and tested on the remaining one (blue boxes), as follows (image courtsey of [Scikit-Learn Documentation](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation)):\n",
    "\n",
    "![alt text](figures/cv.jpg)\n",
    "\n",
    "- Scikit-learn provides us with convenient functions to do CV\n",
    "\n",
    "### Standard Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "eaf10798-dae4-4194-b7cb-4ccff4deb560",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [10, 11], [-1, -2], [0, 0], [3, 3]])\n",
    "y = np.array([1, 2, 3, 4, 5, 6, 7, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "79542ac3-118f-4a15-b368-231e19f1df2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=5, random_state=43, shuffle=True)\n",
      "TRAIN index: [0 1 2 4 5 6] TEST index: [3 7]\n",
      "TRAIN index: [0 1 2 3 4 7] TEST index: [5 6]\n",
      "TRAIN index: [0 3 4 5 6 7] TEST index: [1 2]\n",
      "TRAIN index: [1 2 3 4 5 6 7] TEST index: [0]\n",
      "TRAIN index: [0 1 2 3 5 6 7] TEST index: [4]\n"
     ]
    }
   ],
   "source": [
    "# try using n_splits greater than the number of data points\n",
    "kf = sklearn.model_selection.KFold(n_splits=5, shuffle=True, random_state=43)\n",
    "print(kf)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN index:\", train_index, \"TEST index:\", test_index)\n",
    "    \n",
    "    # build training and testing splits\n",
    "    Xtrain, ytrain = X[train_index,:], y[train_index]\n",
    "    Xtest, ytest = X[test_index,:], y[test_index]\n",
    "    \n",
    "    # train and evaluate the model ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d1fe1c-16ed-47e8-9725-0264816f3ae4",
   "metadata": {},
   "source": [
    "### Stratified Splitting\n",
    "\n",
    "- If you are dealing with a classification problem, it is possible that standard KFold generates training and testing class distributions that are too different.\n",
    "- To solve this issue, scikit-learn provides the `StratifiedKFold` class which ensures that the class distributions are as close as possible in training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "3a1ce05a-6a62-4e32-9570-cf1ee09fe29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train distribution:  [0.05075  0.05125  0.096875 0.699125 0.102   ]\n",
      "Test distribution:   [0.051 0.051 0.097 0.699 0.102]\n",
      "\n",
      "Train distribution:  [0.05075  0.05125  0.096875 0.699125 0.102   ]\n",
      "Test distribution:   [0.051 0.051 0.097 0.699 0.102]\n",
      "\n",
      "Train distribution:  [0.05075  0.05125  0.096875 0.699125 0.102   ]\n",
      "Test distribution:   [0.051 0.051 0.097 0.699 0.102]\n",
      "\n",
      "Train distribution:  [0.050875 0.051125 0.096875 0.699125 0.102   ]\n",
      "Test distribution:   [0.0505 0.0515 0.097  0.699  0.102 ]\n",
      "\n",
      "Train distribution:  [0.050875 0.051125 0.097    0.699    0.102   ]\n",
      "Test distribution:   [0.0505 0.0515 0.0965 0.6995 0.102 ]\n"
     ]
    }
   ],
   "source": [
    "# generate some dummy data, input does not matter, it is the number of instances that matters\n",
    "X = np.zeros((10000, 2))\n",
    "\n",
    "# generate classes according to a categorical distribution with p=[0.05, 0.05, 0.1, 0.7, 0.1]\n",
    "# this will be one-hot-encoded\n",
    "y = rng.multinomial(1, [0.05, 0.05, 0.1, 0.7, 0.1], 10000)\n",
    "\n",
    "# go back to regular class labels [3, 1, 0, ..]\n",
    "y_class = np.argmax(y, axis=1)\n",
    "\n",
    "# standard version\n",
    "kf = sklearn.model_selection.KFold(n_splits=5, shuffle=True, random_state=43)\n",
    "\n",
    "# stratified version\n",
    "kf = sklearn.model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=43)\n",
    "\n",
    "for train_index, test_index in kf.split(X, y_class):\n",
    "    \n",
    "    # build training and testing splits\n",
    "    Xtrain, ytrain = X[train_index,:], y[train_index]\n",
    "    Xtest, ytest = X[test_index,:], y[test_index]\n",
    "    \n",
    "    print()\n",
    "    print(\"Train distribution: \", np.mean(ytrain, axis=0))\n",
    "    print(\"Test distribution:  \", np.mean(ytest, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30d4592-e4a9-4b0f-96af-873c20209f46",
   "metadata": {},
   "source": [
    "### Grouped Splitting\n",
    "\n",
    "- You may encounter a scenario where observations belong to groups. For example, a dataset that characterizes student performance on different subjects will have many obervations belonging to a single student. In this scenario, the student represents the \"group\".\n",
    "- In such cases, it can be desirable to split the dataset such that no group occurs in both training and testing sets, to ensure that the model can't do well by simply memorizing the identity of the group.\n",
    "- In the students example, we'd split the dataset such that observations belonging to a student occur in the training or testing sets, but not both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "60ad1244-a07c-4fc6-a7ef-b4fffacdec25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [0 1] TEST: [2 3]\n",
      "Train groups:  [0 0] , Test groups:  [2 2]\n",
      "TRAIN: [2 3] TEST: [0 1]\n",
      "Train groups:  [2 2] , Test groups:  [0 0]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "y = np.array([1, 2, 3, 4])\n",
    "groups = np.array([0, 0, 2, 2])\n",
    "group_kfold = sklearn.model_selection.GroupKFold(n_splits=2)\n",
    "for train_index, test_index in group_kfold.split(X, y, groups):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    print(\"Train groups: \", groups[train_index], \", Test groups: \", groups[test_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc52829-5e30-4957-b8a5-84fb46d27179",
   "metadata": {},
   "source": [
    "### Example: Performance of multiclass classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "28ec128f-52e4-45a6-aacd-50527339cf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(Xtrain, Ytrain, n_hidden=10, l2_penalty=0.05, epochs=100, verbose=True, activation='relu', batch_size_divisor=10):\n",
    "    keras.backend.clear_session()\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            layers.InputLayer(input_shape=(Xtrain.shape[1],)),\n",
    "            layers.Dense(n_hidden, activation=activation, kernel_regularizer=reg.l2(l2_penalty)),\n",
    "            layers.Dense(Ytrain.shape[1], activation=\"softmax\", kernel_regularizer=reg.l2(l2_penalty)),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    batch_size = Xtrain.shape[0] // batch_size_divisor\n",
    "    \n",
    "    h = model.fit(x = Xtrain, y=Ytrain, verbose=verbose, epochs=epochs, batch_size=batch_size)\n",
    "    \n",
    "    return model, h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "1b48e47b-7176-4e97-b6a4-66f419e6e210",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('data/multiclass_classification_hard.npz')\n",
    "X, y = data['X'], data['y']\n",
    "Y = tf.keras.utils.to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "aff0222c-e4ea-404c-813e-6f6d86a6eca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Mohammad\\miniconda3\\envs\\nncourse\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py:2070: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n"
     ]
    }
   ],
   "source": [
    "# initialize splitter\n",
    "kf = sklearn.model_selection.KFold(n_splits=5, shuffle=True, random_state=43)\n",
    "\n",
    "# keep track of test performance\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "test_cms = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    \n",
    "    # build training and testing splits\n",
    "    Xtrain, Ytrain = X[train_index,:], Y[train_index,:]\n",
    "    Xtest, Ytest = X[test_index,:], Y[test_index,:]\n",
    "    model, history = train_model(Xtrain, Ytrain, verbose=False)\n",
    "    \n",
    "    Yhat = model.predict(Xtest, batch_size=Xtest.shape[0])\n",
    "    \n",
    "    loss = -np.mean(np.sum(Ytest * np.log(Yhat), axis=1))\n",
    "    test_loss.append(loss)\n",
    "    \n",
    "    Yhat_hard = np.argmax(Yhat, axis=1)\n",
    "    acc = sklearn.metrics.accuracy_score(y[test_index], Yhat_hard) \n",
    "    test_acc.append(acc)\n",
    "    \n",
    "    cm = sklearn.metrics.confusion_matrix(y[test_index], Yhat_hard)\n",
    "    test_cms.append(cm)\n",
    "    \n",
    "    print(\"Finished split %d\" % len(test_loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "bc7d82ad-2c53-4fcf-8f3d-b08041714cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss: 0.45 (stderr  0.01), accuracy: 0.86 (stderr 0.02)\n",
      "Mean confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[33.2,  0. ,  0. ,  0. ,  0. ,  0.2],\n",
       "       [ 0. , 31.2,  0. ,  2.2,  0. ,  0. ],\n",
       "       [ 0.4,  0. , 32.6,  0. ,  0.4,  0. ],\n",
       "       [ 0. ,  4.6,  0. , 27.8,  0. ,  1. ],\n",
       "       [ 2.8,  1.2,  8.6,  1. , 15.4,  4.2],\n",
       "       [ 0. ,  0. ,  0. ,  1. ,  0. , 32.2]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = len(test_loss)\n",
    "print(\"Mean loss: %0.2f (stderr  %0.2f), accuracy: %0.2f (stderr %0.2f)\" % (np.mean(test_loss), np.std(test_loss)/np.sqrt(n), np.mean(test_acc), np.std(test_acc) / np.sqrt(n)))\n",
    "print(\"Mean confusion matrix:\")\n",
    "np.mean(test_cms, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d49761-379d-471c-a85d-132115a2bf72",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "- Set the regularizer to 1 and re-run the example, but happens to mean accuracy?\n",
    "- Switch to a stratified KFold sampler, do results change?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7e5ca8-7ea5-4643-828d-ff0f88a37134",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization\n",
    "\n",
    "- As you may have noticed, designing a neural network model involves many decisions: \n",
    "    - Learning rate\n",
    "    - Regularization strength\n",
    "    - Mini-batch size\n",
    "    - Number of hidden layers\n",
    "    - Number of neurons in a layer\n",
    "    - Activation functions\n",
    "    - Dropout rate\n",
    "    - ...\n",
    "- Just like the weights in a neural network model, those \"decisions\" are considered to be free parameters and are explicitly labeled as \"hyperparameters\" -- higher-level parameters of the model\n",
    "- Unfortunately, hyperparameters cannot be learned via gradient descent so some other method is required to determine them\n",
    "- The simplest way to is to pick a few candidate options for each hyperparameter and try all possible combinations of them, this is known as brute force or grid-search hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "77675ab7-6efe-4402-b6ad-827364311220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['batch_size_divisor', 'l2_penalty', 'n_hidden']\n",
      "[[10], [0.001, 0.01, 0.1], [1, 5, 10]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'batch_size_divisor': 10, 'l2_penalty': 0.001, 'n_hidden': 1},\n",
       " {'batch_size_divisor': 10, 'l2_penalty': 0.001, 'n_hidden': 5},\n",
       " {'batch_size_divisor': 10, 'l2_penalty': 0.001, 'n_hidden': 10},\n",
       " {'batch_size_divisor': 10, 'l2_penalty': 0.01, 'n_hidden': 1},\n",
       " {'batch_size_divisor': 10, 'l2_penalty': 0.01, 'n_hidden': 5},\n",
       " {'batch_size_divisor': 10, 'l2_penalty': 0.01, 'n_hidden': 10},\n",
       " {'batch_size_divisor': 10, 'l2_penalty': 0.1, 'n_hidden': 1},\n",
       " {'batch_size_divisor': 10, 'l2_penalty': 0.1, 'n_hidden': 5},\n",
       " {'batch_size_divisor': 10, 'l2_penalty': 0.1, 'n_hidden': 10}]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparams = {\n",
    "    'l2_penalty' : [0.001, 0.01, 0.1],\n",
    "    'n_hidden' : [1, 5, 10],\n",
    "    'batch_size_divisor' : [10]\n",
    "}\n",
    "\n",
    "# get a sorted list of hyperparameters\n",
    "keys = sorted(hyperparams.keys())\n",
    "print(keys)\n",
    "\n",
    "# get the corresponding values\n",
    "vals = [hyperparams[k] for k in keys]\n",
    "print(vals)\n",
    "\n",
    "# generate all combinations of those values\n",
    "all_combs = itertools.product(*vals)\n",
    "\n",
    "# attach key names to them\n",
    "hyperparam_combs = []\n",
    "for comb in all_combs:\n",
    "    hyperparam_combs.append({ k: v for k, v in zip(keys, comb) })\n",
    "    \n",
    "hyperparam_combs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcbff46-fc17-4588-bb89-b22e4661ba43",
   "metadata": {},
   "source": [
    "- In conjunction with CV, we can do brute force hyperparameter optimization as follows: for each combination of hyperparameters, compute the average test loss via CV, and pick the combination that has the smallest loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "804461ff-9eb2-4241-aa25-7c5798c402d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.001, 'n_hidden': 1}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "New best: loss=1.20, acc=0.44\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.001, 'n_hidden': 5}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "New best: loss=0.33, acc=0.88\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.001, 'n_hidden': 10}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "New best: loss=0.29, acc=0.89\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.01, 'n_hidden': 1}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.01, 'n_hidden': 5}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.01, 'n_hidden': 10}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "New best: loss=0.27, acc=0.90\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.1, 'n_hidden': 1}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.1, 'n_hidden': 5}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.1, 'n_hidden': 10}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n"
     ]
    }
   ],
   "source": [
    "# initialize splitter\n",
    "kf = sklearn.model_selection.KFold(n_splits=5, shuffle=True, random_state=43)\n",
    "\n",
    "min_loss = np.inf\n",
    "best_hyperparam = None\n",
    "\n",
    "for hyperparam_comb in hyperparam_combs:\n",
    "    \n",
    "    # keep track of test performance\n",
    "    test_loss = []\n",
    "    test_acc = []\n",
    "    \n",
    "    print(\"Evaluating combination: \", hyperparam_comb)\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "\n",
    "        # build training and testing splits\n",
    "        Xtrain, Ytrain = X[train_index,:], Y[train_index,:]\n",
    "        Xtest, Ytest = X[test_index,:], Y[test_index,:]\n",
    "        model, history = train_model(Xtrain, Ytrain, verbose=False, epochs=200, **hyperparam_comb)\n",
    "\n",
    "        Yhat = model.predict(Xtest, batch_size=Xtest.shape[0])\n",
    "\n",
    "        loss = -np.mean(np.sum(Ytest * np.log(Yhat), axis=1))\n",
    "        test_loss.append(loss)\n",
    "\n",
    "        Yhat_hard = np.argmax(Yhat, axis=1)\n",
    "        acc = sklearn.metrics.accuracy_score(y[test_index], Yhat_hard) \n",
    "        test_acc.append(acc)\n",
    "\n",
    "        cm = sklearn.metrics.confusion_matrix(y[test_index], Yhat_hard)\n",
    "        test_cms.append(cm)\n",
    "\n",
    "        print(\"Finished split %d\" % len(test_loss))\n",
    "    \n",
    "    mean_loss = np.mean(test_loss)\n",
    "    mean_acc = np.mean(test_acc)\n",
    "    \n",
    "    if mean_loss < min_loss:\n",
    "        min_loss = mean_loss \n",
    "        best_hyperparam = hyperparam_comb\n",
    "        print(\"New best: loss=%0.2f, acc=%0.2f\" % (mean_loss, mean_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "8eab9972-4c15-448f-bdbb-116c7f37ae9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size_divisor': 10, 'l2_penalty': 0.01, 'n_hidden': 10}"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hyperparam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b184c48c-5d1b-4422-993c-324d6be6b3ac",
   "metadata": {},
   "source": [
    "## Nested Cross-Validation\n",
    "\n",
    "- After performing hyperparameter optimization above, you may be inclined to report the CV test performance of the best performing combination\n",
    "- But that is not appropriate, because CV itself was used to select the best hyperparameter combination\n",
    "- In ML parlance, we say that the test data _leaked_ into training\n",
    "- The correct approach to do this is to consider the hyperparameter optimization as part of the model training\n",
    "- This gives rise to _nested cross-validation_ which consists of two nested loops\n",
    "- The outer loop performs K-Fold cross-validation as usual\n",
    "- The innter loop performs K-Fold cross-validation on the ___training set___ given by the outer loop\n",
    "- First, let's refactor our code above so that it is easy to use within nested cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "a7f7899a-387e-49e4-8fe7-bef57710e891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparam_optimize(X, Y, hyperparam_combs, splits=5):\n",
    "    \n",
    "    # initialize splitter\n",
    "    kf = sklearn.model_selection.KFold(n_splits=splits, shuffle=True)\n",
    "\n",
    "    min_loss = np.inf\n",
    "    best_hyperparam = None\n",
    "\n",
    "    for hyperparam_comb in hyperparam_combs:\n",
    "\n",
    "        # keep track of test performance\n",
    "        test_loss = []\n",
    "        test_acc = []\n",
    "\n",
    "        print(\"Evaluating combination: \", hyperparam_comb)\n",
    "\n",
    "        for train_index, test_index in kf.split(X):\n",
    "\n",
    "            # build training and testing splits\n",
    "            Xtrain, Ytrain = X[train_index,:], Y[train_index,:]\n",
    "            Xtest, Ytest = X[test_index,:], Y[test_index,:]\n",
    "            model, history = train_model(Xtrain, Ytrain, verbose=False, epochs=200, **hyperparam_comb)\n",
    "\n",
    "            Yhat = model.predict(Xtest, batch_size=Xtest.shape[0])\n",
    "\n",
    "            loss = -np.mean(np.sum(Ytest * np.log(Yhat), axis=1))\n",
    "            test_loss.append(loss)\n",
    "\n",
    "            Yhat_hard = np.argmax(Yhat, axis=1)\n",
    "            acc = sklearn.metrics.accuracy_score(y[test_index], Yhat_hard) \n",
    "            test_acc.append(acc)\n",
    "\n",
    "            cm = sklearn.metrics.confusion_matrix(y[test_index], Yhat_hard)\n",
    "            test_cms.append(cm)\n",
    "\n",
    "            print(\"Finished split %d\" % len(test_loss))\n",
    "\n",
    "        mean_loss = np.mean(test_loss)\n",
    "        mean_acc = np.mean(test_acc)\n",
    "\n",
    "        if mean_loss < min_loss:\n",
    "            min_loss = mean_loss \n",
    "            best_hyperparam = hyperparam_comb\n",
    "            print(\"New best: loss=%0.2f, acc=%0.2f\" % (mean_loss, mean_acc))\n",
    "    \n",
    "    # now we can train the model on whole dataset that is given to this function\n",
    "    model, history = train_model(X, Y, verbose=False, epochs=200, **best_hyperparam)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cab036-65cb-4c9f-9b41-8af43a9b0884",
   "metadata": {},
   "source": [
    "Now we can implement nested CV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "c8d961bc-ffee-4b65-b733-393d64fec1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.001, 'n_hidden': 1}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "New best: loss=1.24, acc=0.14\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.001, 'n_hidden': 5}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "New best: loss=0.37, acc=0.14\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.001, 'n_hidden': 10}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "New best: loss=0.31, acc=0.14\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.01, 'n_hidden': 1}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.01, 'n_hidden': 5}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.01, 'n_hidden': 10}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "New best: loss=0.30, acc=0.15\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.1, 'n_hidden': 1}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0010s). Check your callbacks.\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.1, 'n_hidden': 5}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.1, 'n_hidden': 10}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "Finished split 1\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.001, 'n_hidden': 1}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "New best: loss=1.24, acc=0.18\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.001, 'n_hidden': 5}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "New best: loss=0.41, acc=0.17\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.001, 'n_hidden': 10}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "New best: loss=0.27, acc=0.17\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.01, 'n_hidden': 1}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.01, 'n_hidden': 5}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.01, 'n_hidden': 10}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.1, 'n_hidden': 1}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.1, 'n_hidden': 5}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.1, 'n_hidden': 10}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "Finished split 2\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.001, 'n_hidden': 1}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "New best: loss=1.24, acc=0.16\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.001, 'n_hidden': 5}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "New best: loss=0.40, acc=0.19\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.001, 'n_hidden': 10}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0009s). Check your callbacks.\n",
      "Finished split 5\n",
      "New best: loss=0.30, acc=0.19\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.01, 'n_hidden': 1}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.01, 'n_hidden': 5}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.01, 'n_hidden': 10}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.1, 'n_hidden': 1}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.1, 'n_hidden': 5}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.1, 'n_hidden': 10}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "Finished split 3\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.001, 'n_hidden': 1}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "New best: loss=1.19, acc=0.15\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.001, 'n_hidden': 5}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_begin` time: 0.0010s). Check your callbacks.\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "New best: loss=0.32, acc=0.17\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.001, 'n_hidden': 10}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "New best: loss=0.30, acc=0.18\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.01, 'n_hidden': 1}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.01, 'n_hidden': 5}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.01, 'n_hidden': 10}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "New best: loss=0.27, acc=0.17\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.1, 'n_hidden': 1}\n",
      "Finished split 1\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_begin` time: 0.0001s). Check your callbacks.\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.1, 'n_hidden': 5}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0010s). Check your callbacks.\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.1, 'n_hidden': 10}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "Finished split 4\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.001, 'n_hidden': 1}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "New best: loss=1.22, acc=0.15\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.001, 'n_hidden': 5}\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0010s). Check your callbacks.\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "New best: loss=0.38, acc=0.19\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.001, 'n_hidden': 10}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "New best: loss=0.28, acc=0.18\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.01, 'n_hidden': 1}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0010s). Check your callbacks.\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.01, 'n_hidden': 5}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.01, 'n_hidden': 10}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "New best: loss=0.27, acc=0.18\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.1, 'n_hidden': 1}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.1, 'n_hidden': 5}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0050s). Check your callbacks.\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "Evaluating combination:  {'batch_size_divisor': 10, 'l2_penalty': 0.1, 'n_hidden': 10}\n",
      "Finished split 1\n",
      "Finished split 2\n",
      "Finished split 3\n",
      "Finished split 4\n",
      "Finished split 5\n",
      "Finished split 5\n"
     ]
    }
   ],
   "source": [
    "# initialize splitter\n",
    "kf = sklearn.model_selection.KFold(n_splits=5, shuffle=True, random_state=43)\n",
    "\n",
    "# keep track of test performance\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "test_cms = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    \n",
    "    # build training and testing splits\n",
    "    Xtrain, Ytrain = X[train_index,:], Y[train_index,:]\n",
    "    Xtest, Ytest = X[test_index,:], Y[test_index,:]\n",
    "    \n",
    "    # now we perform hyper parameter optimization on the TRAINING SET only\n",
    "    model, history = hyperparam_optimize(Xtrain, Ytrain, hyperparam_combs)\n",
    "    \n",
    "    # predict\n",
    "    Yhat = model.predict(Xtest, batch_size=Xtest.shape[0])\n",
    "    \n",
    "    loss = -np.mean(np.sum(Ytest * np.log(Yhat), axis=1))\n",
    "    test_loss.append(loss)\n",
    "    \n",
    "    Yhat_hard = np.argmax(Yhat, axis=1)\n",
    "    acc = sklearn.metrics.accuracy_score(y[test_index], Yhat_hard) \n",
    "    test_acc.append(acc)\n",
    "    \n",
    "    cm = sklearn.metrics.confusion_matrix(y[test_index], Yhat_hard)\n",
    "    test_cms.append(cm)\n",
    "    \n",
    "    print(\"Finished split %d\" % len(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "88c4abcf-352a-4834-9e88-190d2f35f7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss: 0.28 (stderr  0.01), accuracy: 0.91 (stderr 0.01)\n",
      "Mean confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[5.77391304, 4.93043478, 3.6826087 , 4.17826087, 3.07826087,\n",
       "        4.71304348],\n",
       "       [5.35652174, 5.96521739, 4.19565217, 4.05652174, 2.48695652,\n",
       "        4.29565217],\n",
       "       [5.52608696, 5.54347826, 5.13043478, 4.40434783, 2.87391304,\n",
       "        4.44347826],\n",
       "       [5.03478261, 4.74782609, 3.99130435, 4.6       , 2.57391304,\n",
       "        4.43043478],\n",
       "       [5.46956522, 5.36956522, 4.54782609, 4.00434783, 3.52173913,\n",
       "        4.80869565],\n",
       "       [5.03043478, 5.06086957, 4.16521739, 4.36521739, 3.52608696,\n",
       "        4.98695652]])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = len(test_loss)\n",
    "print(\"Mean loss: %0.2f (stderr  %0.2f), accuracy: %0.2f (stderr %0.2f)\" % (np.mean(test_loss), np.std(test_loss)/np.sqrt(n), np.mean(test_acc), np.std(test_acc) / np.sqrt(n)))\n",
    "print(\"Mean confusion matrix:\")\n",
    "np.mean(test_cms, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5947e1dc-b2b6-46b8-a3cc-9f71564acf91",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "- When you have a large number of input features, you might be interested in identifying which ones are important for prediction\n",
    "- Typically, when you are exploring the input features, you'd remove duplicated features that are highly correlated or anti-correlated with each other\n",
    "- After that, you may still have a large number of features to choose from\n",
    "- One possible selection strategy is to exhaustively enumerate all feature combinations and chose the subset of features that performs best on test, or has the fewest features with the least performance deficit\n",
    "- Another strategy is to greedily add the best features one at a time, which is much cheaper than exhaustive search\n",
    "- The optimal set of features of a model should be thought of as a hyper parameter of the model\n",
    "- But doing feature selection and hyper parameter optimization at the same time, while ideal, can be **_very_** expensive so one strategy is to first identify a good set of hyperparameters, then perform feature selection\n",
    "- Let's implement the forward greedy selection procedure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "2a476f7c-5667-4341-b28e-2a0542bfbd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_greedy_feature_selection(X, Y, features, thres):\n",
    "    \n",
    "    # randomly split the data into training and testing\n",
    "    # you could make this more elaborate and use CV here\n",
    "    # but because this is just a demo, we're going to use a single \n",
    "    # training/testing split\n",
    "    Xtrain, Xtest, Ytrain, Ytest = sklearn.model_selection.train_test_split(X, Y, test_size=0.2)\n",
    "    \n",
    "    selected_features = set()\n",
    "    \n",
    "    all_features = set(features)\n",
    "    \n",
    "    curr_loss = None \n",
    "    \n",
    "    while len(selected_features) < len(features):\n",
    "        \n",
    "        best_feature = None\n",
    "        best_loss = np.inf \n",
    "        \n",
    "        rem_features = all_features - selected_features\n",
    "        \n",
    "        # for each remaining feature\n",
    "        for feature in rem_features:\n",
    "            \n",
    "            # form the candidate feature subset (existing features + the new feature)\n",
    "            candidate_set = list(selected_features) + [feature]\n",
    "            \n",
    "            # train the model\n",
    "            model, history = train_model(Xtrain[:, candidate_set], Ytrain, verbose=False, epochs=200, l2_penalty=0.01, n_hidden=10)\n",
    "\n",
    "            # evaluate it\n",
    "            Yhat = model.predict(Xtest[:, candidate_set], batch_size=Xtest.shape[0])\n",
    "            \n",
    "            # check the loss\n",
    "            loss = -np.mean(np.sum(Ytest * np.log(Yhat), axis=1))\n",
    "            \n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                best_feature = feature\n",
    "        \n",
    "            print(\"Finished \", feature)\n",
    "        \n",
    "        if curr_loss is not None:\n",
    "            rel_loss = ( (curr_loss - best_loss) / curr_loss )\n",
    "            print(\"Relative loss of best feature: %f, Absolute loss=%f\" % (rel_loss, best_loss))\n",
    "        # add the best performing feature if we are just starting out\n",
    "        if curr_loss is None:\n",
    "            selected_features.add(best_feature)\n",
    "            \n",
    "            curr_loss = best_loss \n",
    "            \n",
    "        # add the best performing feature if it decreases the loss by more than thres%\n",
    "        elif best_loss < curr_loss and ( (curr_loss - best_loss) / curr_loss ) >= thres:\n",
    "            \n",
    "            selected_features.add(best_feature)\n",
    "            curr_loss = best_loss\n",
    "        \n",
    "        # otherwise, terminate the process\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # train the model\n",
    "    selected_features = list(selected_features)\n",
    "    \n",
    "    model, history = train_model(X[:,selected_features], Y, verbose=False, epochs=200, l2_penalty=0.01, n_hidden=10)\n",
    "\n",
    "    return model, history, selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "d3c27c19-ce41-4c53-9770-0daf620eda66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished  0\n",
      "Finished  1\n",
      "Finished  2\n",
      "Finished  3\n",
      "Finished  4\n",
      "Finished  5\n",
      "Finished  6\n",
      "Finished  7\n",
      "Finished  8\n",
      "Finished  9\n",
      "Finished  10\n",
      "Finished  11\n",
      "Finished  1\n",
      "Finished  2\n",
      "Finished  3\n",
      "Finished  4\n",
      "Finished  5\n",
      "Finished  6\n",
      "Finished  7\n",
      "Finished  8\n",
      "Finished  9\n",
      "Finished  10\n",
      "Finished  11\n",
      "Relative loss of best feature: 0.646144, Absolute loss=0.274795\n",
      "Finished  2\n",
      "Finished  3\n",
      "Finished  4\n",
      "Finished  5\n",
      "Finished  6\n",
      "Finished  7\n",
      "Finished  8\n",
      "Finished  9\n",
      "Finished  10\n",
      "Finished  11\n",
      "Relative loss of best feature: 0.071689, Absolute loss=0.255095\n",
      "Finished  2\n",
      "Finished  3\n",
      "Finished  4\n",
      "Finished  6\n",
      "Finished  7\n",
      "Finished  8\n",
      "Finished  9\n",
      "Finished  10\n",
      "Finished  11\n",
      "Relative loss of best feature: -0.036151, Absolute loss=0.264317\n",
      "Selected:  [0, 1, 5]\n",
      "Finished split 1\n",
      "Finished  0\n",
      "Finished  1\n",
      "Finished  2\n",
      "Finished  3\n",
      "Finished  4\n",
      "Finished  5\n",
      "Finished  6\n",
      "Finished  7\n",
      "Finished  8\n",
      "Finished  9\n",
      "Finished  10\n",
      "Finished  11\n",
      "Finished  1\n",
      "Finished  2\n",
      "Finished  3\n",
      "Finished  4\n",
      "Finished  5\n",
      "Finished  6\n",
      "Finished  7\n",
      "Finished  8\n",
      "Finished  9\n",
      "Finished  10\n",
      "Finished  11\n",
      "Relative loss of best feature: 0.732608, Absolute loss=0.218869\n",
      "Finished  2\n",
      "Finished  3\n",
      "Finished  4\n",
      "Finished  5\n",
      "Finished  6\n",
      "Finished  7\n",
      "Finished  8\n",
      "Finished  9\n",
      "Finished  10\n",
      "Finished  11\n",
      "Relative loss of best feature: 0.104641, Absolute loss=0.195967\n",
      "Finished  2\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0125s). Check your callbacks.\n",
      "Finished  4\n",
      "Finished  5\n",
      "Finished  6\n",
      "Finished  7\n",
      "Finished  8\n",
      "Finished  9\n",
      "Finished  10\n",
      "Finished  11\n",
      "Relative loss of best feature: -0.016771, Absolute loss=0.199253\n",
      "Selected:  [0, 1, 3]\n",
      "Finished split 2\n",
      "Finished  0\n",
      "Finished  1\n",
      "Finished  2\n",
      "Finished  3\n",
      "Finished  4\n",
      "Finished  5\n",
      "Finished  6\n",
      "Finished  7\n",
      "Finished  8\n",
      "Finished  9\n",
      "Finished  10\n",
      "Finished  11\n",
      "Finished  1\n",
      "Finished  2\n",
      "Finished  3\n",
      "Finished  4\n",
      "Finished  5\n",
      "Finished  6\n",
      "Finished  7\n",
      "Finished  8\n",
      "Finished  9\n",
      "Finished  10\n",
      "Finished  11\n",
      "Relative loss of best feature: 0.708180, Absolute loss=0.246848\n",
      "Finished  2\n",
      "Finished  3\n",
      "Finished  4\n",
      "Finished  5\n",
      "Finished  6\n",
      "Finished  7\n",
      "Finished  8\n",
      "Finished  9\n",
      "Finished  10\n",
      "Finished  11\n",
      "Relative loss of best feature: 0.013905, Absolute loss=0.243415\n",
      "Selected:  [0, 1]\n",
      "Finished split 3\n",
      "Finished  0\n",
      "Finished  1\n",
      "Finished  2\n",
      "Finished  3\n",
      "Finished  4\n",
      "Finished  5\n",
      "Finished  6\n",
      "Finished  7\n",
      "Finished  8\n",
      "Finished  9\n",
      "Finished  10\n",
      "Finished  11\n",
      "Finished  1\n",
      "Finished  2\n",
      "Finished  3\n",
      "Finished  4\n",
      "Finished  5\n",
      "Finished  6\n",
      "Finished  7\n",
      "Finished  8\n",
      "Finished  9\n",
      "Finished  10\n",
      "Finished  11\n",
      "Relative loss of best feature: 0.688235, Absolute loss=0.245415\n",
      "Finished  2\n",
      "Finished  3\n",
      "Finished  4\n",
      "Finished  5\n",
      "Finished  6\n",
      "Finished  7\n",
      "Finished  8\n",
      "Finished  9\n",
      "Finished  10\n",
      "Finished  11\n",
      "Relative loss of best feature: 0.000283, Absolute loss=0.245346\n",
      "Selected:  [0, 1]\n",
      "Finished split 4\n",
      "Finished  0\n",
      "Finished  1\n",
      "Finished  2\n",
      "Finished  3\n",
      "Finished  4\n",
      "Finished  5\n",
      "Finished  6\n",
      "Finished  7\n",
      "Finished  8\n",
      "Finished  9\n",
      "Finished  10\n",
      "Finished  11\n",
      "Finished  1\n",
      "Finished  2\n",
      "Finished  3\n",
      "Finished  4\n",
      "Finished  5\n",
      "Finished  6\n",
      "Finished  7\n",
      "Finished  8\n",
      "Finished  9\n",
      "Finished  10\n",
      "Finished  11\n",
      "Relative loss of best feature: 0.661950, Absolute loss=0.281733\n",
      "Finished  2\n",
      "Finished  3\n",
      "Finished  4\n",
      "Finished  5\n",
      "Finished  6\n",
      "Finished  7\n",
      "Finished  8\n",
      "Finished  9\n",
      "Finished  10\n",
      "Finished  11\n",
      "Relative loss of best feature: 0.076972, Absolute loss=0.260047\n",
      "Finished  2\n",
      "Finished  3\n",
      "Finished  4\n",
      "Finished  5\n",
      "Finished  6\n",
      "Finished  7\n",
      "Finished  8\n",
      "Finished  10\n",
      "Finished  11\n",
      "Relative loss of best feature: -0.040626, Absolute loss=0.270612\n",
      "Selected:  [0, 1, 9]\n",
      "Finished split 5\n"
     ]
    }
   ],
   "source": [
    "# initialize splitter\n",
    "kf = sklearn.model_selection.KFold(n_splits=5, shuffle=True, random_state=43)\n",
    "\n",
    "# keep track of test performance\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "test_cms = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    \n",
    "    # build training and testing splits\n",
    "    Xtrain, Ytrain = X[train_index,:], Y[train_index,:]\n",
    "    Xtest, Ytest = X[test_index,:], Y[test_index,:]\n",
    "    \n",
    "    model, history, selected_features = forward_greedy_feature_selection(Xtrain, Ytrain, range(X.shape[1]), 0.05)\n",
    "    print(\"Selected: \", selected_features)\n",
    "    \n",
    "    # predict\n",
    "    Yhat = model.predict(Xtest[:,selected_features], batch_size=Xtest.shape[0])\n",
    "    \n",
    "    loss = -np.mean(np.sum(Ytest * np.log(Yhat), axis=1))\n",
    "    test_loss.append(loss)\n",
    "    \n",
    "    Yhat_hard = np.argmax(Yhat, axis=1)\n",
    "    acc = sklearn.metrics.accuracy_score(y[test_index], Yhat_hard) \n",
    "    test_acc.append(acc)\n",
    "    \n",
    "    cm = sklearn.metrics.confusion_matrix(y[test_index], Yhat_hard)\n",
    "    test_cms.append(cm)\n",
    "    \n",
    "    print(\"Finished split %d\" % len(test_loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "95937169-457c-45e2-8ec8-ea3daf0da6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss: 0.26 (stderr  0.00), accuracy: 0.92 (stderr 0.01)\n",
      "Mean confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[33.2,  0. ,  0. ,  0. ,  0.2,  0. ],\n",
       "       [ 0. , 31.6,  0. ,  1.8,  0. ,  0. ],\n",
       "       [ 0.2,  0. , 31.4,  0. ,  1.8,  0. ],\n",
       "       [ 0. ,  2.6,  0. , 29. ,  0.8,  1. ],\n",
       "       [ 0.4,  0. ,  3.8,  0.8, 27. ,  1.2],\n",
       "       [ 0. ,  0. ,  0. ,  1.2,  1.2, 30.8]])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = len(test_loss)\n",
    "print(\"Mean loss: %0.2f (stderr  %0.2f), accuracy: %0.2f (stderr %0.2f)\" % (np.mean(test_loss), np.std(test_loss)/np.sqrt(n), np.mean(test_acc), np.std(test_acc) / np.sqrt(n)))\n",
    "print(\"Mean confusion matrix:\")\n",
    "np.mean(test_cms, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ecf8ca-1c8f-4408-8755-8cedf6731efb",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "- Cross-validation is used to estimate the generalization performance of ML models\n",
    "- Hyperparameter optimization is concerned with finding optimal high-level parameters of a NN (parameters that are not trained with gradient descent)\n",
    "- It is important not to report model performance based CV results obtained from hyperparameter optimization\n",
    "- Nested cross-validation solves this problem by treating the hyperparameter optimization as part of the model itself, and performing it on each training set generated by CV\n",
    "- Feature selection is concerned with finding the optimal subset of features for prediction\n",
    "- Greedy forward feature selection adds one feature at a time until some criterion is met (i.e., minimum improvement percentage)\n",
    "- Like hyper parameter optimization, feature selection should only be performed on the training set, ideally via nested cross-validation\n",
    "- The point is: NEVER touch the test set until you are ready. That means don't use it for hyper parameter optimization or feature selection.\n",
    "- To be safe when starting a data science project, split your data into development and testing. Do all your model development on the development set, and do one final test on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d997787a-530c-4abc-9c0c-40254f91abe5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
